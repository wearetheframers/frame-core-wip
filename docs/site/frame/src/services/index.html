<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.1">
<title>frame.src.services API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>frame.src.services</code></h1>
</header>
<section id="section-intro">
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="frame.src.services.context" href="context/index.html">frame.src.services.context</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="frame.src.services.eq" href="eq/index.html">frame.src.services.eq</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="frame.src.services.execution_context" href="execution_context.html">frame.src.services.execution_context</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="frame.src.services.llm" href="llm/index.html">frame.src.services.llm</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="frame.src.services.memory" href="memory/index.html">frame.src.services.memory</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="frame.src.services.EQService"><code class="flex name class">
<span>class <span class="ident">EQService</span></span>
</code></dt>
<dd>
<div class="desc"><p>EQ (Emotional Intelligence) Service for managing emotional intelligence capabilities.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EQService:
    &#34;&#34;&#34;
    EQ (Emotional Intelligence) Service for managing emotional intelligence capabilities.
    &#34;&#34;&#34;

    def __init__(self):
        self.adapter = SentimentAnalysisAdapter()

    def analyze_text(self, text: str):
        &#34;&#34;&#34;
        Analyze the given text for emotional content.

        Args:
            text (str): The text to analyze.

        Returns:
            dict: The emotional state detected in the text.
        &#34;&#34;&#34;
        return self.adapter.analyze(text)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="frame.src.services.EQService.analyze_text"><code class="name flex">
<span>def <span class="ident">analyze_text</span></span>(<span>self, text: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Analyze the given text for emotional content.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>text</code></strong> :&ensp;<code>str</code></dt>
<dd>The text to analyze.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>The emotional state detected in the text.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="frame.src.services.ExecutionContext"><code class="flex name class">
<span>class <span class="ident">ExecutionContext</span></span>
<span>(</span><span>llm_service: "'<a title="frame.src.services.LLMService" href="#frame.src.services.LLMService">LLMService</a>'", memory_service: "Optional['<a title="frame.src.services.MemoryService" href="#frame.src.services.MemoryService">MemoryService</a>']" = None, eq_service: "Optional['<a title="frame.src.services.EQService" href="#frame.src.services.EQService">EQService</a>']" = None, soul: "Optional['Soul']" = None, state: Dict[str, Any] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>ExecutionContext provides a centralized container for various services, state, and functions
that actions and components might need during execution. It ensures consistent access to resources
across all actions and facilitates easier testing, modular design, and state management.</p>
<p>The ExecutionContext serves as a bridge between different components of the Framer,
allowing them to share resources and functionality without tight coupling.</p>
<p>Key features:
- Centralized service access: Provides access to core services like LLM, memory, and EQ.
- State management: Maintains and updates the current state of the execution.
- Goal tracking: Manages the current goals of the Framer.
- Action registry: Stores and manages available actions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ExecutionContext:
    &#34;&#34;&#34;
    ExecutionContext provides a centralized container for various services, state, and functions
    that actions and components might need during execution. It ensures consistent access to resources
    across all actions and facilitates easier testing, modular design, and state management.

    The ExecutionContext serves as a bridge between different components of the Framer,
    allowing them to share resources and functionality without tight coupling.

    Key features:
    - Centralized service access: Provides access to core services like LLM, memory, and EQ.
    - State management: Maintains and updates the current state of the execution.
    - Goal tracking: Manages the current goals of the Framer.
    - Action registry: Stores and manages available actions.
    &#34;&#34;&#34;

    def __init__(
        self,
        llm_service: &#34;LLMService&#34;,
        memory_service: Optional[&#34;MemoryService&#34;] = None,
        eq_service: Optional[&#34;EQService&#34;] = None,
        soul: Optional[&#34;Soul&#34;] = None,
        state: Dict[str, Any] = None,
    ):
        self.llm_service = llm_service
        self.memory_service = memory_service
        self.eq_service = eq_service
        self.soul = soul
        self.state = state or {}
        self.goals: List[Any] = []
        self.roles: List[Any] = []
        self.action_registry = None

    def set_state(self, key: str, value: Any) -&gt; None:
        self.state[key] = value

    def get_state(self, key: str, default: Any = None) -&gt; Any:
        return self.state.get(key, default)

    def get_llm_service(self) -&gt; LLMService:
        return self.llm_service

    def get_memory_service(self) -&gt; Optional[MemoryService]:
        return self.memory_service

    def get_eq_service(self) -&gt; Optional[EQService]:
        return self.eq_service

    def get_soul(self) -&gt; Optional[Soul]:
        return self.soul

    def update_state(self, new_state: Dict[str, Any]) -&gt; None:
        self.state.update(new_state)

    def get_full_state(self) -&gt; Dict[str, Any]:
        return self.state.copy()

    def set_goals(self, goals: List[Any]):
        &#34;&#34;&#34;
        Set the current goals for the Framer.

        Args:
            goals (List[Any]): A list of Goal objects to set as the current goals.
        &#34;&#34;&#34;
        self.goals = goals

    def get_goals(self) -&gt; List[Any]:
        &#34;&#34;&#34;
        Get the current goals of the Framer.

        Returns:
            List[Any]: A list of the current Goal objects.
        &#34;&#34;&#34;
        return self.goals

    def set_roles(self, roles: List[Any]):
        &#34;&#34;&#34;
        Set the current roles for the Framer.

        Args:
            roles (List[Any]): A list of Role objects to set as the current roles.
        &#34;&#34;&#34;
        self.roles = roles

    def get_roles(self) -&gt; List[Any]:
        &#34;&#34;&#34;
        Get the current roles of the Framer.

        Returns:
            List[Any]: A list of the current Role objects.
        &#34;&#34;&#34;
        return self.roles

    async def generate_goals(self) -&gt; List[Any]:
        &#34;&#34;&#34;
        Generate new goals for the Framer.

        Returns:
            List[Any]: A list of newly generated Goal objects.
        &#34;&#34;&#34;
        # This is a placeholder implementation. You should implement the actual goal generation logic here.
        # For now, we&#39;ll return an empty list.
        return []

    async def perform_task(self, task: Dict[str, Any]) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;
        Perform a task using the LLM service.

        Args:
            task (Dict[str, Any]): A dictionary containing task details.

        Returns:
            Dict[str, Any]: The result of the task execution.
        &#34;&#34;&#34;
        # This is a simple implementation. You might want to expand this based on your specific needs.
        prompt = f&#34;Perform the following task: {task[&#39;description&#39;]}&#34;
        response = await self.llm_service.get_completion(prompt)
        return {&#34;output&#34;: response}

    async def generate_goals(self) -&gt; List[Any]:
        &#34;&#34;&#34;
        Generate new goals for the Framer.

        Returns:
            List[Any]: A list of newly generated Goal objects.
        &#34;&#34;&#34;
        # This is a placeholder implementation. You should implement the actual goal generation logic here.
        # For now, we&#39;ll return an empty list.
        return []</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="frame.src.services.ExecutionContext.generate_goals"><code class="name flex">
<span>async def <span class="ident">generate_goals</span></span>(<span>self) ‑> List[Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Generate new goals for the Framer.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[Any]</code></dt>
<dd>A list of newly generated Goal objects.</dd>
</dl></div>
</dd>
<dt id="frame.src.services.ExecutionContext.get_eq_service"><code class="name flex">
<span>def <span class="ident">get_eq_service</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.ExecutionContext.get_full_state"><code class="name flex">
<span>def <span class="ident">get_full_state</span></span>(<span>self) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.ExecutionContext.get_goals"><code class="name flex">
<span>def <span class="ident">get_goals</span></span>(<span>self) ‑> List[Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Get the current goals of the Framer.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[Any]</code></dt>
<dd>A list of the current Goal objects.</dd>
</dl></div>
</dd>
<dt id="frame.src.services.ExecutionContext.get_llm_service"><code class="name flex">
<span>def <span class="ident">get_llm_service</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.ExecutionContext.get_memory_service"><code class="name flex">
<span>def <span class="ident">get_memory_service</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.ExecutionContext.get_roles"><code class="name flex">
<span>def <span class="ident">get_roles</span></span>(<span>self) ‑> List[Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Get the current roles of the Framer.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[Any]</code></dt>
<dd>A list of the current Role objects.</dd>
</dl></div>
</dd>
<dt id="frame.src.services.ExecutionContext.get_soul"><code class="name flex">
<span>def <span class="ident">get_soul</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.ExecutionContext.get_state"><code class="name flex">
<span>def <span class="ident">get_state</span></span>(<span>self, key: str, default: Any = None) ‑> Any</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.ExecutionContext.perform_task"><code class="name flex">
<span>async def <span class="ident">perform_task</span></span>(<span>self, task: Dict[str, Any]) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Perform a task using the LLM service.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>task</code></strong> :&ensp;<code>Dict[str, Any]</code></dt>
<dd>A dictionary containing task details.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, Any]</code></dt>
<dd>The result of the task execution.</dd>
</dl></div>
</dd>
<dt id="frame.src.services.ExecutionContext.set_goals"><code class="name flex">
<span>def <span class="ident">set_goals</span></span>(<span>self, goals: List[Any])</span>
</code></dt>
<dd>
<div class="desc"><p>Set the current goals for the Framer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>goals</code></strong> :&ensp;<code>List[Any]</code></dt>
<dd>A list of Goal objects to set as the current goals.</dd>
</dl></div>
</dd>
<dt id="frame.src.services.ExecutionContext.set_roles"><code class="name flex">
<span>def <span class="ident">set_roles</span></span>(<span>self, roles: List[Any])</span>
</code></dt>
<dd>
<div class="desc"><p>Set the current roles for the Framer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>roles</code></strong> :&ensp;<code>List[Any]</code></dt>
<dd>A list of Role objects to set as the current roles.</dd>
</dl></div>
</dd>
<dt id="frame.src.services.ExecutionContext.set_state"><code class="name flex">
<span>def <span class="ident">set_state</span></span>(<span>self, key: str, value: Any) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.ExecutionContext.update_state"><code class="name flex">
<span>def <span class="ident">update_state</span></span>(<span>self, new_state: Dict[str, Any]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="frame.src.services.LLMService"><code class="flex name class">
<span>class <span class="ident">LLMService</span></span>
<span>(</span><span>openai_api_key: str = None, mistral_api_key: str = None, huggingface_api_key: str = None, default_model: str = None, metrics: <a title="frame.src.utils.llm_utils.LLMMetrics" href="../utils/llm_utils.html#frame.src.utils.llm_utils.LLMMetrics">LLMMetrics</a> = None)</span>
</code></dt>
<dd>
<div class="desc"><p>LLMService is responsible for managing interactions with various language models.
It provides methods to set the default model and generate text completions using
the specified or default model.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>openai_api_key</code></strong> :&ensp;<code>str</code></dt>
<dd>API key for OpenAI services.</dd>
<dt><strong><code>mistral_api_key</code></strong> :&ensp;<code>str</code></dt>
<dd>API key for Mistral services.</dd>
<dt><strong><code>huggingface_api_key</code></strong> :&ensp;<code>str</code></dt>
<dd>API key for Hugging Face services.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LLMService:
    &#34;&#34;&#34;
    LLMService is responsible for managing interactions with various language models.
    It provides methods to set the default model and generate text completions using
    the specified or default model.

    Attributes:
        openai_api_key (str): API key for OpenAI services.
        mistral_api_key (str): API key for Mistral services.
        huggingface_api_key (str): API key for Hugging Face services.
    &#34;&#34;&#34;

    def __init__(
        self,
        openai_api_key: str = None,
        mistral_api_key: str = None,
        huggingface_api_key: str = None,
        default_model: str = None,
        metrics: LLMMetrics = None,
    ):
        self.logger = logging.getLogger(__name__)
        self.openai_api_key = openai_api_key or OPENAI_API_KEY
        self.mistral_api_key = mistral_api_key
        self.huggingface_api_key = huggingface_api_key or HUGGINGFACE_API_KEY
        self.default_model = default_model or (
            &#34;gpt-3.5-turbo&#34; if self.openai_api_key else &#34;mistral-medium&#34;
        )
        self.metrics = metrics or llm_metrics
        self._adapters = {}

    def get_adapter(self, model_name: str):
        if model_name not in self._adapters:
            if &#34;gpt&#34; in model_name.lower():
                self._adapters[model_name] = LMQLAdapter(
                    openai_api_key=self.openai_api_key
                )
            elif &#34;mistral&#34; in model_name.lower():
                self._adapters[model_name] = LMQLAdapter(
                    mistral_api_key=self.mistral_api_key
                )
            elif &#34;huggingface&#34; in model_name.lower():
                self._adapters[model_name] = HuggingFaceAdapter(
                    huggingface_api_key=self.huggingface_api_key
                )
            else:
                raise ValueError(f&#34;Unsupported model: {model_name}&#34;)
        return self._adapters[model_name]

    def get_metrics(self) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;
        Get the current LLM usage metrics.

        Returns:
            Dict[str, Any]: A dictionary containing the call count and cost for each model,
            as well as the total calls and total cost.
        &#34;&#34;&#34;
        return self.metrics.get_metrics()

    def get_total_calls(self) -&gt; int:
        &#34;&#34;&#34;
        Get the total number of LLM calls made.

        Returns:
            int: The total number of calls made.
        &#34;&#34;&#34;
        return self.metrics._total_calls

    def get_total_cost(self) -&gt; float:
        &#34;&#34;&#34;
        Get the total cost of LLM usage.

        Returns:
            float: The total cost incurred.
        &#34;&#34;&#34;
        return self.metrics._total_cost

    def _prepare_full_prompt(
        self,
        prompt: str,
        include_frame_context: bool,
        recent_memories: Optional[List[Dict[str, Any]]],
    ) -&gt; str:
        full_prompt = prompt

        if include_frame_context:
            frame_context = &#34;&#34;&#34;
            Frame is a multi-modal cognitive agent framework designed to support fully emergent characteristics.
            It consists of three main components: Frame, Framed, and Framer.
            - Frame: The main interface for creating and managing Framer instances.
            - Framer: An individual AI agent with capabilities for task management, decision-making, and interaction with language models.
            - Framed: A collection of Framer objects working together to achieve complex tasks.
            &#34;&#34;&#34;
            full_prompt = f&#34;{frame_context}\n\n{full_prompt}&#34;

        if recent_memories:
            memories_context = &#34;Recent memories and perceptions:\n&#34;
            for memory in recent_memories:
                memories_context += (
                    f&#34;- {memory.get(&#39;type&#39;, &#39;Memory&#39;)}: {memory.get(&#39;content&#39;, &#39;&#39;)}\n&#34;
                )
            full_prompt = f&#34;{memories_context}\n{full_prompt}&#34;

        return full_prompt

    def set_default_model(self, model: str):
        &#34;&#34;&#34;
        Set the default model to be used for all operations.

        Args:
            model (str): The model to set as default.
        &#34;&#34;&#34;
        self.default_model = model
        self.logger.info(f&#34;Default model set to: {self.default_model}&#34;)

    async def get_completion(
        self,
        prompt: str,
        model: str = None,
        max_tokens: int = 1024,
        temperature: float = 0.7,
        additional_context: Optional[Dict[str, Any]] = None,
        expected_output: Optional[str] = None,
        use_local: bool = False,
        stream: bool = False,
        include_frame_context: bool = False,
        recent_memories: Optional[List[Dict[str, Any]]] = None,
    ) -&gt; str:
        model = model or self.default_model
        self.logger.debug(f&#34;Using model: {model}&#34;)

        start_time = time.time()

        full_prompt = self._prepare_full_prompt(
            prompt, include_frame_context, recent_memories
        )

        try:
            adapter = self.get_adapter(model)
            config = adapter.get_config(max_tokens=max_tokens, temperature=temperature)
            formatted_prompt = adapter.format_prompt(full_prompt)
            result = await adapter.get_completion(
                formatted_prompt, config, additional_context
            )

            end_time = time.time()
            execution_time = end_time - start_time
            tokens_used = len(full_prompt.split()) + (
                len(result.split()) if isinstance(result, str) else 0
            )

            self.metrics.track_usage(model, tokens_used)

            self.logger.debug(f&#34;Completion generated in {execution_time:.2f} seconds&#34;)
            self.logger.debug(f&#34;Tokens used: {tokens_used}&#34;)

            if isinstance(result, dict):
                return json.dumps(result)
            elif isinstance(result, str):
                return result
            else:
                return str(result)

        except Exception as e:
            self.logger.error(f&#34;Error in get_completion: {str(e)}&#34;)
            return f&#34;I encountered an error while processing your request: {str(e)}. Could you please try again?&#34;
        &#34;&#34;&#34;
        Generate a completion using the specified model or the default model.

        Args:
            prompt (str): The input prompt for the model.
            model (str, optional): The model to use.
            max_tokens (int, optional): Maximum number of tokens to generate.
            temperature (float, optional): Sampling temperature.
            additional_context (Dict[str, Any], optional): Additional context for the model.
            expected_output (str, optional): Expected output format.
            use_local (bool, optional): Whether to use a local model.
            stream (bool, optional): Whether to stream the output.
            include_frame_context (bool, optional): Whether to include context about Frame and Framers.
            recent_memories (List[Dict[str, Any]], optional): List of recent memories/perceptions.

        Returns:
            Union[str, Dict[str, Any]]: The generated completion.
        &#34;&#34;&#34;
        model = model or self.default_model
        self.logger.debug(f&#34;Using model: {model}&#34;)

        start_time = time.time()

        # Prepare the full prompt with additional context if required
        full_prompt = self._prepare_full_prompt(
            prompt, include_frame_context, recent_memories
        )

        result = None  # Initialize result to avoid UnboundLocalError

        try:
            if stream:
                # ... (streaming code remains unchanged)
                pass
            else:
                if use_local or (
                    self.huggingface_adapter.api_key and &#34;huggingface&#34; in model.lower()
                ):
                    self.logger.debug(&#34;Using Hugging Face adapter&#34;)
                    config = HuggingFaceConfig(
                        model=model, max_tokens=max_tokens, temperature=temperature
                    )
                    formatted_prompt = format_huggingface_prompt(full_prompt)
                    result = await self.huggingface_adapter.get_completion(
                        formatted_prompt, config, additional_context
                    )
                elif &#34;dspy&#34; in model.lower():
                    self.logger.debug(&#34;Using DSPy adapter&#34;)
                    config = DSPyConfig(
                        model=model, max_tokens=max_tokens, temperature=temperature
                    )
                    formatted_prompt = format_dspy_prompt(full_prompt)
                    result = await self.dspy_wrapper.get_completion(
                        formatted_prompt, config, additional_context
                    )
                else:
                    self.logger.debug(&#34;Using LMQL adapter&#34;)
                    config = LMQLConfig(
                        model=model, max_tokens=max_tokens, temperature=temperature
                    )
                    constraints = (
                        [f&#34;EXPECTED_OUTPUT in [{expected_output}]&#34;]
                        if expected_output
                        else []
                    )
                    if &#34;lmql&#34; in model.lower():
                        result = await self.lmql_interface.generate(
                            full_prompt, max_tokens=max_tokens, constraints=constraints
                        )
                    elif &#34;dspy&#34; in model.lower():
                        config = DSPyConfig(
                            model=model, max_tokens=max_tokens, temperature=temperature
                        )
                        formatted_prompt = format_dspy_prompt(full_prompt)
                        result = await self.dspy_wrapper.get_completion(
                            formatted_prompt, config, additional_context
                        )

            end_time = time.time()
            execution_time = end_time - start_time

            # Calculate tokens used (this is a simple estimation)
            if isinstance(result, str):
                tokens_used = len(prompt.split()) + len(result.split())
            else:
                tokens_used = len(
                    prompt.split()
                )  # We can&#39;t estimate tokens for streaming result

            # Track LLM usage
            track_llm_usage(model, tokens_used)

            self.logger.debug(f&#34;Completion generated in {execution_time:.2f} seconds&#34;)
            self.logger.debug(f&#34;Tokens used: {tokens_used}&#34;)

            if (
                result is None
                or (isinstance(result, str) and not result.strip())
                or (isinstance(result, dict) and &#34;error&#34; in result)
            ):
                self.logger.warning(f&#34;Received invalid response from model: {model}&#34;)
                return {
                    &#34;error&#34;: f&#34;Invalid response from model: {model}&#34;,
                    &#34;fallback_response&#34;: &#34;I apologize, but I couldn&#39;t generate a response at this time. Could you please rephrase your question or provide more context?&#34;,
                }

            if isinstance(result, dict):
                return result
            elif isinstance(result, str):
                try:
                    return json.loads(result)
                except json.JSONDecodeError:
                    return {&#34;response&#34;: result}

            # Check if ```json is in the response and remove it if present
            if (
                isinstance(result, str)
                and result.startswith(&#34;```json&#34;)
                and result.endswith(&#34;```&#34;)
            ):
                try:
                    cleaned_response = &#34;\n&#34;.join(result.split(&#34;\n&#34;)[1:-1])
                    return json.loads(cleaned_response)
                except json.JSONDecodeError:
                    return result

            return result

        except Exception as e:
            self.logger.error(f&#34;Error in get_completion: {str(e)}&#34;)
            return {
                &#34;error&#34;: str(e),
                &#34;fallback_response&#34;: &#34;I encountered an error while processing your request. Could you please try again?&#34;,
            }</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="frame.src.services.LLMService.get_adapter"><code class="name flex">
<span>def <span class="ident">get_adapter</span></span>(<span>self, model_name: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.LLMService.get_completion"><code class="name flex">
<span>async def <span class="ident">get_completion</span></span>(<span>self, prompt: str, model: str = None, max_tokens: int = 1024, temperature: float = 0.7, additional_context: Optional[Dict[str, Any]] = None, expected_output: Optional[str] = None, use_local: bool = False, stream: bool = False, include_frame_context: bool = False, recent_memories: Optional[List[Dict[str, Any]]] = None) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.LLMService.get_metrics"><code class="name flex">
<span>def <span class="ident">get_metrics</span></span>(<span>self) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Get the current LLM usage metrics.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, Any]</code></dt>
<dd>A dictionary containing the call count and cost for each model,</dd>
</dl>
<p>as well as the total calls and total cost.</p></div>
</dd>
<dt id="frame.src.services.LLMService.get_total_calls"><code class="name flex">
<span>def <span class="ident">get_total_calls</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Get the total number of LLM calls made.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>The total number of calls made.</dd>
</dl></div>
</dd>
<dt id="frame.src.services.LLMService.get_total_cost"><code class="name flex">
<span>def <span class="ident">get_total_cost</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Get the total cost of LLM usage.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>The total cost incurred.</dd>
</dl></div>
</dd>
<dt id="frame.src.services.LLMService.set_default_model"><code class="name flex">
<span>def <span class="ident">set_default_model</span></span>(<span>self, model: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the default model to be used for all operations.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>str</code></dt>
<dd>The model to set as default.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="frame.src.services.LocalContext"><code class="flex name class">
<span>class <span class="ident">LocalContext</span></span>
<span>(</span><span>soul: Optional[Any] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Local Context Service Module</p>
<p>This module provides the LocalContext class, which manages local context
for agent components, allowing them to share information with each other.
It serves as a base class for more specialized local contexts.</p>
<p>Initialize the Context.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>soul</code></strong> :&ensp;<code>Optional[Any]</code></dt>
<dd>The soul attribute for the context.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LocalContext:
    &#34;&#34;&#34;
    Local Context Service Module

    This module provides the LocalContext class, which manages local context
    for agent components, allowing them to share information with each other.
    It serves as a base class for more specialized local contexts.
    &#34;&#34;&#34;

    def __init__(self, soul: Optional[Any] = None, **kwargs):
        &#34;&#34;&#34;
        Initialize the Context.

        Args:
            soul (Optional[Any]): The soul attribute for the context.
        &#34;&#34;&#34;
        self._data = {
            &#34;soul&#34;: soul,
            &#34;state&#34;: kwargs,
            &#34;history&#34;: [],
            &#34;roles&#34;: [],
            &#34;goals&#34;: [],
        }
        for key, value in kwargs.items():
            self._data[key] = value

    def add_to_history(self, entry: str):
        &#34;&#34;&#34;
        Add an entry to the history.

        Args:
            entry (str): The entry to add to the history.
        &#34;&#34;&#34;
        self._data[&#34;history&#34;].append(entry)

    def __setattr__(self, name, value):
        if name.startswith(&#34;_&#34;):
            super().__setattr__(name, value)
        else:
            self._data[name] = value

    def __getattr__(self, name):
        if name in self._data:
            return self._data[name]
        raise AttributeError(
            f&#34;&#39;{self.__class__.__name__}&#39; object has no attribute &#39;{name}&#39;&#34;
        )

    def get(self, key, default=None):
        return self._data.get(key, default)

    def get_roles(self) -&gt; List[Dict[str, Any]]:
        return self._data[&#34;roles&#34;]

    def set_roles(self, roles: List[Dict[str, Any]]) -&gt; None:
        self._data[&#34;roles&#34;] = roles

    def get_goals(self) -&gt; List[Dict[str, Any]]:
        return self._data[&#34;goals&#34;]

    def set_goals(self, goals: List[Dict[str, Any]]) -&gt; None:
        self._data[&#34;goals&#34;] = goals

    def set_soul(self, soul: Any) -&gt; None:
        &#34;&#34;&#34;
        Set the soul for the context.

        Args:
            soul (Any): The soul to set.
        &#34;&#34;&#34;
        self._data[&#34;soul&#34;] = soul</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="frame.src.services.context.shared_context_service.SharedContext" href="context/shared_context_service.html#frame.src.services.context.shared_context_service.SharedContext">SharedContext</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="frame.src.services.LocalContext.add_to_history"><code class="name flex">
<span>def <span class="ident">add_to_history</span></span>(<span>self, entry: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Add an entry to the history.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>entry</code></strong> :&ensp;<code>str</code></dt>
<dd>The entry to add to the history.</dd>
</dl></div>
</dd>
<dt id="frame.src.services.LocalContext.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self, key, default=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.LocalContext.get_goals"><code class="name flex">
<span>def <span class="ident">get_goals</span></span>(<span>self) ‑> List[Dict[str, Any]]</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.LocalContext.get_roles"><code class="name flex">
<span>def <span class="ident">get_roles</span></span>(<span>self) ‑> List[Dict[str, Any]]</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.LocalContext.set_goals"><code class="name flex">
<span>def <span class="ident">set_goals</span></span>(<span>self, goals: List[Dict[str, Any]]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.LocalContext.set_roles"><code class="name flex">
<span>def <span class="ident">set_roles</span></span>(<span>self, roles: List[Dict[str, Any]]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.LocalContext.set_soul"><code class="name flex">
<span>def <span class="ident">set_soul</span></span>(<span>self, soul: Any) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Set the soul for the context.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>soul</code></strong> :&ensp;<code>Any</code></dt>
<dd>The soul to set.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="frame.src.services.MemoryService"><code class="flex name class">
<span>class <span class="ident">MemoryService</span></span>
<span>(</span><span>adapter)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MemoryService:
    def __init__(self, adapter):
        self.adapter = adapter

    def store(
        self, memory: str, user_id: str = &#34;default&#34;, metadata: Dict[str, Any] = None
    ) -&gt; int:
        return self.adapter.store(memory, user_id, metadata)

    def retrieve_memory(
        self, memory_id: int, user_id: str = &#34;default&#34;
    ) -&gt; Optional[Any]:
        return self.adapter.retrieve(memory_id, user_id)

    def update_memory(
        self, memory_id: int, data: str, user_id: str = &#34;default&#34;
    ) -&gt; bool:
        return self.adapter.update(memory_id, data, user_id)

    def delete_memory(self, memory_id: int, user_id: str = &#34;default&#34;) -&gt; bool:
        return self.adapter.delete(memory_id, user_id)

    def get_all_memories(self, user_id: str = &#34;default&#34;) -&gt; List[Dict[str, Any]]:
        return self.adapter.get_all(user_id)

    def search_memories(
        self, query: str, user_id: str = &#34;default&#34;
    ) -&gt; List[Dict[str, Any]]:
        return self.adapter.search(query, user_id)

    def get_memory_history(self, memory_id: int, user_id: str = &#34;default&#34;) -&gt; List[str]:
        return self.adapter.history(memory_id, user_id)

    def clear_all_memories(self, user_id: str = &#34;default&#34;) -&gt; bool:
        return self.adapter.clear_all(user_id)

    def get_memory_count(self, user_id: str = &#34;default&#34;) -&gt; int:
        return self.adapter.get_count(user_id)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="frame.src.services.MemoryService.clear_all_memories"><code class="name flex">
<span>def <span class="ident">clear_all_memories</span></span>(<span>self, user_id: str = 'default') ‑> bool</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.MemoryService.delete_memory"><code class="name flex">
<span>def <span class="ident">delete_memory</span></span>(<span>self, memory_id: int, user_id: str = 'default') ‑> bool</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.MemoryService.get_all_memories"><code class="name flex">
<span>def <span class="ident">get_all_memories</span></span>(<span>self, user_id: str = 'default') ‑> List[Dict[str, Any]]</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.MemoryService.get_memory_count"><code class="name flex">
<span>def <span class="ident">get_memory_count</span></span>(<span>self, user_id: str = 'default') ‑> int</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.MemoryService.get_memory_history"><code class="name flex">
<span>def <span class="ident">get_memory_history</span></span>(<span>self, memory_id: int, user_id: str = 'default') ‑> List[str]</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.MemoryService.retrieve_memory"><code class="name flex">
<span>def <span class="ident">retrieve_memory</span></span>(<span>self, memory_id: int, user_id: str = 'default') ‑> Optional[Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.MemoryService.search_memories"><code class="name flex">
<span>def <span class="ident">search_memories</span></span>(<span>self, query: str, user_id: str = 'default') ‑> List[Dict[str, Any]]</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.MemoryService.store"><code class="name flex">
<span>def <span class="ident">store</span></span>(<span>self, memory: str, user_id: str = 'default', metadata: Dict[str, Any] = None) ‑> int</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="frame.src.services.MemoryService.update_memory"><code class="name flex">
<span>def <span class="ident">update_memory</span></span>(<span>self, memory_id: int, data: str, user_id: str = 'default') ‑> bool</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="frame.src.services.SharedContext"><code class="flex name class">
<span>class <span class="ident">SharedContext</span></span>
<span>(</span><span>soul: Optional[Any] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Local Context Service Module</p>
<p>This module provides the LocalContext class, which manages local context
for agent components, allowing them to share information with each other.
It serves as a base class for more specialized local contexts.</p>
<p>Initialize the Context.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>soul</code></strong> :&ensp;<code>Optional[Any]</code></dt>
<dd>The soul attribute for the context.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SharedContext(LocalContext):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="frame.src.services.context.local_context_service.LocalContext" href="context/local_context_service.html#frame.src.services.context.local_context_service.LocalContext">LocalContext</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="frame.src.services.context.local_context_service.LocalContext" href="context/local_context_service.html#frame.src.services.context.local_context_service.LocalContext">LocalContext</a></b></code>:
<ul class="hlist">
<li><code><a title="frame.src.services.context.local_context_service.LocalContext.add_to_history" href="context/local_context_service.html#frame.src.services.context.local_context_service.LocalContext.add_to_history">add_to_history</a></code></li>
<li><code><a title="frame.src.services.context.local_context_service.LocalContext.set_soul" href="context/local_context_service.html#frame.src.services.context.local_context_service.LocalContext.set_soul">set_soul</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="frame.src" href="../index.html">frame.src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="frame.src.services.context" href="context/index.html">frame.src.services.context</a></code></li>
<li><code><a title="frame.src.services.eq" href="eq/index.html">frame.src.services.eq</a></code></li>
<li><code><a title="frame.src.services.execution_context" href="execution_context.html">frame.src.services.execution_context</a></code></li>
<li><code><a title="frame.src.services.llm" href="llm/index.html">frame.src.services.llm</a></code></li>
<li><code><a title="frame.src.services.memory" href="memory/index.html">frame.src.services.memory</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="frame.src.services.EQService" href="#frame.src.services.EQService">EQService</a></code></h4>
<ul class="">
<li><code><a title="frame.src.services.EQService.analyze_text" href="#frame.src.services.EQService.analyze_text">analyze_text</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="frame.src.services.ExecutionContext" href="#frame.src.services.ExecutionContext">ExecutionContext</a></code></h4>
<ul class="two-column">
<li><code><a title="frame.src.services.ExecutionContext.generate_goals" href="#frame.src.services.ExecutionContext.generate_goals">generate_goals</a></code></li>
<li><code><a title="frame.src.services.ExecutionContext.get_eq_service" href="#frame.src.services.ExecutionContext.get_eq_service">get_eq_service</a></code></li>
<li><code><a title="frame.src.services.ExecutionContext.get_full_state" href="#frame.src.services.ExecutionContext.get_full_state">get_full_state</a></code></li>
<li><code><a title="frame.src.services.ExecutionContext.get_goals" href="#frame.src.services.ExecutionContext.get_goals">get_goals</a></code></li>
<li><code><a title="frame.src.services.ExecutionContext.get_llm_service" href="#frame.src.services.ExecutionContext.get_llm_service">get_llm_service</a></code></li>
<li><code><a title="frame.src.services.ExecutionContext.get_memory_service" href="#frame.src.services.ExecutionContext.get_memory_service">get_memory_service</a></code></li>
<li><code><a title="frame.src.services.ExecutionContext.get_roles" href="#frame.src.services.ExecutionContext.get_roles">get_roles</a></code></li>
<li><code><a title="frame.src.services.ExecutionContext.get_soul" href="#frame.src.services.ExecutionContext.get_soul">get_soul</a></code></li>
<li><code><a title="frame.src.services.ExecutionContext.get_state" href="#frame.src.services.ExecutionContext.get_state">get_state</a></code></li>
<li><code><a title="frame.src.services.ExecutionContext.perform_task" href="#frame.src.services.ExecutionContext.perform_task">perform_task</a></code></li>
<li><code><a title="frame.src.services.ExecutionContext.set_goals" href="#frame.src.services.ExecutionContext.set_goals">set_goals</a></code></li>
<li><code><a title="frame.src.services.ExecutionContext.set_roles" href="#frame.src.services.ExecutionContext.set_roles">set_roles</a></code></li>
<li><code><a title="frame.src.services.ExecutionContext.set_state" href="#frame.src.services.ExecutionContext.set_state">set_state</a></code></li>
<li><code><a title="frame.src.services.ExecutionContext.update_state" href="#frame.src.services.ExecutionContext.update_state">update_state</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="frame.src.services.LLMService" href="#frame.src.services.LLMService">LLMService</a></code></h4>
<ul class="two-column">
<li><code><a title="frame.src.services.LLMService.get_adapter" href="#frame.src.services.LLMService.get_adapter">get_adapter</a></code></li>
<li><code><a title="frame.src.services.LLMService.get_completion" href="#frame.src.services.LLMService.get_completion">get_completion</a></code></li>
<li><code><a title="frame.src.services.LLMService.get_metrics" href="#frame.src.services.LLMService.get_metrics">get_metrics</a></code></li>
<li><code><a title="frame.src.services.LLMService.get_total_calls" href="#frame.src.services.LLMService.get_total_calls">get_total_calls</a></code></li>
<li><code><a title="frame.src.services.LLMService.get_total_cost" href="#frame.src.services.LLMService.get_total_cost">get_total_cost</a></code></li>
<li><code><a title="frame.src.services.LLMService.set_default_model" href="#frame.src.services.LLMService.set_default_model">set_default_model</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="frame.src.services.LocalContext" href="#frame.src.services.LocalContext">LocalContext</a></code></h4>
<ul class="two-column">
<li><code><a title="frame.src.services.LocalContext.add_to_history" href="#frame.src.services.LocalContext.add_to_history">add_to_history</a></code></li>
<li><code><a title="frame.src.services.LocalContext.get" href="#frame.src.services.LocalContext.get">get</a></code></li>
<li><code><a title="frame.src.services.LocalContext.get_goals" href="#frame.src.services.LocalContext.get_goals">get_goals</a></code></li>
<li><code><a title="frame.src.services.LocalContext.get_roles" href="#frame.src.services.LocalContext.get_roles">get_roles</a></code></li>
<li><code><a title="frame.src.services.LocalContext.set_goals" href="#frame.src.services.LocalContext.set_goals">set_goals</a></code></li>
<li><code><a title="frame.src.services.LocalContext.set_roles" href="#frame.src.services.LocalContext.set_roles">set_roles</a></code></li>
<li><code><a title="frame.src.services.LocalContext.set_soul" href="#frame.src.services.LocalContext.set_soul">set_soul</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="frame.src.services.MemoryService" href="#frame.src.services.MemoryService">MemoryService</a></code></h4>
<ul class="two-column">
<li><code><a title="frame.src.services.MemoryService.clear_all_memories" href="#frame.src.services.MemoryService.clear_all_memories">clear_all_memories</a></code></li>
<li><code><a title="frame.src.services.MemoryService.delete_memory" href="#frame.src.services.MemoryService.delete_memory">delete_memory</a></code></li>
<li><code><a title="frame.src.services.MemoryService.get_all_memories" href="#frame.src.services.MemoryService.get_all_memories">get_all_memories</a></code></li>
<li><code><a title="frame.src.services.MemoryService.get_memory_count" href="#frame.src.services.MemoryService.get_memory_count">get_memory_count</a></code></li>
<li><code><a title="frame.src.services.MemoryService.get_memory_history" href="#frame.src.services.MemoryService.get_memory_history">get_memory_history</a></code></li>
<li><code><a title="frame.src.services.MemoryService.retrieve_memory" href="#frame.src.services.MemoryService.retrieve_memory">retrieve_memory</a></code></li>
<li><code><a title="frame.src.services.MemoryService.search_memories" href="#frame.src.services.MemoryService.search_memories">search_memories</a></code></li>
<li><code><a title="frame.src.services.MemoryService.store" href="#frame.src.services.MemoryService.store">store</a></code></li>
<li><code><a title="frame.src.services.MemoryService.update_memory" href="#frame.src.services.MemoryService.update_memory">update_memory</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="frame.src.services.SharedContext" href="#frame.src.services.SharedContext">SharedContext</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.1</a>.</p>
</footer>
</body>
</html>
