# Core dependencies for local inference
llama-cpp-python>=0.2.11
torch>=2.1.0
transformers>=4.36.0
accelerate>=0.27.0
bitsandbytes>=0.41.0
